{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4305991,"sourceType":"datasetVersion","datasetId":2536476}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport time\nfrom torchvision import datasets, transforms, models  # datsets  , transforms\nfrom torch.utils.data.sampler import SubsetRandomSampler\nimport torch.nn as nn\nfrom datetime import datetime\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:36:07.370474Z","iopub.execute_input":"2025-03-27T15:36:07.370745Z","iopub.status.idle":"2025-03-27T15:36:13.403054Z","shell.execute_reply.started":"2025-03-27T15:36:07.370724Z","shell.execute_reply":"2025-03-27T15:36:13.402329Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"transform4 = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomRotation(30),\n    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    transforms.RandomApply([transforms.GaussianBlur(kernel_size=5)], p=0.2),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # MobileNet-specific\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:36:15.805929Z","iopub.execute_input":"2025-03-27T15:36:15.806244Z","iopub.status.idle":"2025-03-27T15:36:15.811605Z","shell.execute_reply.started":"2025-03-27T15:36:15.806217Z","shell.execute_reply":"2025-03-27T15:36:15.810785Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"dataset_mobilenet=datasets.ImageFolder(\"/kaggle/input/mepco-tropic-leaf/MepcoTropicLeaf-V1/Database\", transform=transform4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:36:16.985750Z","iopub.execute_input":"2025-03-27T15:36:16.986027Z","iopub.status.idle":"2025-03-27T15:36:23.883887Z","shell.execute_reply.started":"2025-03-27T15:36:16.986005Z","shell.execute_reply":"2025-03-27T15:36:23.882890Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"indices=list(range(len(dataset_mobilenet))) # dataset ko numerate karne ke liye\nsplit=int(np.floor(0.70*len(dataset_mobilenet)))\nvalidation=int(np.floor(0.60*split))\n\n# agar tmre pass 100 samples hai\n# toh split=70(ie 70% of the dataset)-> ee use hoga training and validation ke liye; remaining (30)30% used hoga as test set\n# tb validation=42 (ie 60% of the dataset)-> ee use hoga for training ke liye; remaining (28)40% used hoga as Validation set\n# toh phir training= 42%; validation= 28%; test=30%\n\nprint(f\"length of train size : {validation}\")\nprint(f\"length of validation size : {split-validation}\")\nprint(f\"length of test size : {len(dataset_mobilenet)-split}\")\n\nnp.random.shuffle(indices) # dataset me randomness laane ke liye\n\n# ab actual splitting\ntrain_indices, validation_indices, test_indices = (\n    indices[:validation], # [:5]->0,1,2,3,4\n    indices[validation:split],# [1:3]->1,2\n    indices[split:],# [2:]-> 2,3,4,5,........\n)\n\ntrain_sampler = SubsetRandomSampler(train_indices)\nvalidation_sampler = SubsetRandomSampler(validation_indices)\ntest_sampler = SubsetRandomSampler(test_indices)\n\n# print(list(train_indices))\n# print(list(validation_indices))\n# print(list(test_indices))\n\n# print(list(train_sampler))\n# print(list(validation_sampler))\n# print(list(test_sampler))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:36:27.702386Z","iopub.execute_input":"2025-03-27T15:36:27.702746Z","iopub.status.idle":"2025-03-27T15:36:27.709581Z","shell.execute_reply.started":"2025-03-27T15:36:27.702721Z","shell.execute_reply":"2025-03-27T15:36:27.708813Z"}},"outputs":[{"name":"stdout","text":"length of train size : 1585\nlength of validation size : 1058\nlength of test size : 1134\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"targets_size = len(dataset_mobilenet.class_to_idx)# finding the total unique classes and storing it\nprint(targets_size)\nprint(list(dataset_mobilenet.class_to_idx.keys()))\nnum_classes_list = list(dataset_mobilenet.class_to_idx.values())# now numerating them\nprint(num_classes_list)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:36:30.717579Z","iopub.execute_input":"2025-03-27T15:36:30.717887Z","iopub.status.idle":"2025-03-27T15:36:30.723267Z","shell.execute_reply.started":"2025-03-27T15:36:30.717863Z","shell.execute_reply":"2025-03-27T15:36:30.722384Z"}},"outputs":[{"name":"stdout","text":"50\n['Asthma Plant.zip', 'Avaram.zip', 'Balloon vine.zip', 'Bellyache bush (Green).zip', 'Benghal dayflower.zip', 'Big Caltrops.zip', 'Black-Honey Shrub.zip', 'Bristly Wild Grape.zip', 'Butterfly Pea.zip', 'Cape Gooseberry.zip', 'Common Wireweed.zip', 'Country Mallow.zip', 'Crown flower.zip', 'Green Chireta.zip', 'Holy Basil.zip', 'Indian CopperLeaf.zip', 'Indian Jujube.zip', 'Indian Sarsaparilla.zip', 'Indian Stinging Nettle.zip', 'Indian Thornapple.zip', 'Indian wormwood.zip', 'Ivy Gourd.zip', 'Kokilaksha.zip', 'Land Caltrops (Bindii).zip', 'Madagascar Periwinkle.zip', 'Madras Pea Pumpkin.zip', 'Malabar Catmint.zip', 'Mexican Mint.zip', 'Mexican Prickly Poppy.zip', 'Mountain Knotgrass.zip', 'Nalta Jute.zip', 'Night blooming Cereus.zip', 'Panicled Foldwing.zip', 'Prickly Chaff Flower.zip', 'Punarnava.zip', 'Purple Fruited Pea Eggplant.zip', 'Purple Tephrosia.zip', 'Rosary Pea.zip', 'Shaggy button weed.zip', 'Small Water Clover.zip', 'Spiderwisp.zip', 'Square Stalked Vine.zip', 'Stinking Passionflower.zip', 'Sweet Basil.zip', 'Sweet flag.zip', 'Tinnevelly Senna.zip', 'Trellis Vine.zip', 'Velvet bean.zip', 'coatbuttons.zip', 'heart-leaved moonseed.zip']\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"model2 = models.mobilenet_v2(pretrained=True)\n\n# Freeze all layers first\nfor param in model2.parameters():\n    param.requires_grad = False\n\n# Unfreeze last 3 inverted residual blocks (adjust as needed)\nfor layer in model2.features[-6:]:  # Last 6 layers\n    for param in layer.parameters():\n        param.requires_grad = True\n\n# Modify classifier\nmodel2.classifier[1] = nn.Linear(model2.classifier[1].in_features, targets_size)\nmodel2.classifier[0] = nn.Dropout(p=0.3)  # Reduce dropout from 0.5 to 0.3\n\nmodel2.classifier = nn.Sequential(\n    nn.Dropout(0.3),\n    nn.Linear(model2.classifier[1].in_features, 512),  # Hidden layer\n    nn.ReLU(),\n    nn.Linear(512, targets_size))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:36:31.001142Z","iopub.execute_input":"2025-03-27T15:36:31.001422Z","iopub.status.idle":"2025-03-27T15:36:31.421635Z","shell.execute_reply.started":"2025-03-27T15:36:31.001379Z","shell.execute_reply":"2025-03-27T15:36:31.420826Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n100%|██████████| 13.6M/13.6M [00:00<00:00, 112MB/s] \n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"n_features = model2.classifier[1].in_features #mobilenet; number of input features in the first fully connected layer\nn_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:36:35.009620Z","iopub.execute_input":"2025-03-27T15:36:35.009936Z","iopub.status.idle":"2025-03-27T15:36:35.016311Z","shell.execute_reply.started":"2025-03-27T15:36:35.009913Z","shell.execute_reply":"2025-03-27T15:36:35.015581Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"1280"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device) #checking if GPU is available\nmodel2.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:36:35.827772Z","iopub.execute_input":"2025-03-27T15:36:35.828104Z","iopub.status.idle":"2025-03-27T15:36:36.113527Z","shell.execute_reply.started":"2025-03-27T15:36:35.828075Z","shell.execute_reply":"2025-03-27T15:36:36.112803Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"MobileNetV2(\n  (features): Sequential(\n    (0): Conv2dNormActivation(\n      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU6(inplace=True)\n    )\n    (1): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (2): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (3): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (4): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (5): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (6): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (7): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (8): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (9): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (10): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (11): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (12): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (13): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (14): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (15): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (16): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (17): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (18): Conv2dNormActivation(\n      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU6(inplace=True)\n    )\n  )\n  (classifier): Sequential(\n    (0): Dropout(p=0.3, inplace=False)\n    (1): Linear(in_features=1280, out_features=512, bias=True)\n    (2): ReLU()\n    (3): Linear(in_features=512, out_features=50, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()  # this include softmax + cross entropy loss\n# calculates the loss during training, which will be later used by backpropagation to imporove the models accuracy\noptimizer = torch.optim.Adam([\n    {'params': model2.features[-6:].parameters(), 'lr': 1e-4},  # Higher LR for unfrozen\n    {'params': model2.classifier.parameters(), 'lr': 1e-3}       # Even higher for classifier\n], weight_decay=1e-5)  # Reduce weight decay #Add to the optimizer to penalize large weights\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n#adam optimiser is used to optimise the models parameters(weights of the model) to minimise the loss and hence increase the accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:36:42.121716Z","iopub.execute_input":"2025-03-27T15:36:42.121995Z","iopub.status.idle":"2025-03-27T15:36:42.126980Z","shell.execute_reply.started":"2025-03-27T15:36:42.121975Z","shell.execute_reply":"2025-03-27T15:36:42.126035Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"batch_size = 64\ntrain_loader = torch.utils.data.DataLoader(\n    dataset_mobilenet, batch_size=batch_size, sampler=train_sampler\n)\nvalidation_loader = torch.utils.data.DataLoader(\n    dataset_mobilenet, batch_size=batch_size, sampler=validation_sampler\n)\ntest_loader = torch.utils.data.DataLoader(\n    dataset_mobilenet, batch_size=batch_size, sampler=test_sampler\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:36:44.701053Z","iopub.execute_input":"2025-03-27T15:36:44.701375Z","iopub.status.idle":"2025-03-27T15:36:44.706159Z","shell.execute_reply.started":"2025-03-27T15:36:44.701349Z","shell.execute_reply":"2025-03-27T15:36:44.705411Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def plot_losses(train_losses, test_losses):\n    plt.figure(figsize=(10, 6))\n    plt.plot(train_losses, label='Training Loss', color='blue', marker='o')\n    plt.plot(test_losses, label='Validation Loss', color='red', marker='x')\n    plt.title('Training and Validation Losses', fontsize=16)\n    plt.xlabel('Epochs', fontsize=12)\n    plt.ylabel('Loss', fontsize=12)\n    plt.legend()\n    plt.grid(True, linestyle='--', alpha=0.7)\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:36:46.938084Z","iopub.execute_input":"2025-03-27T15:36:46.938446Z","iopub.status.idle":"2025-03-27T15:36:46.944237Z","shell.execute_reply.started":"2025-03-27T15:36:46.938379Z","shell.execute_reply":"2025-03-27T15:36:46.943202Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def batch_gd_new(model2, criterion, train_loader, test_loader, epochs):\n    train_losses = np.zeros(epochs)\n    test_losses = np.zeros(epochs)\n    \n    # Memory tracking initialization\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        mem_before = torch.cuda.memory_allocated() / 1024**2  # MB\n    \n    # Track average inference time\n    total_inference_time = 0\n    inference_count = 0\n    \n    for e in range(epochs):\n        t0 = datetime.now()\n        train_loss = []\n        \n        # Training phase\n        for inputs, targets in train_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            \n            # Measure inference time\n            start_time = time.time()\n            optimizer.zero_grad()\n            output = model2(inputs)\n            inference_time = time.time() - start_time\n            \n            total_inference_time += inference_time\n            inference_count += 1\n            \n            loss = criterion(output, targets)\n            train_loss.append(loss.item())\n            loss.backward()\n            optimizer.step()\n\n        train_loss = np.mean(train_loss)\n        test_loss = []\n        \n        # Validation phase\n        model2.eval()\n        with torch.no_grad():\n            for inputs, targets in test_loader:\n                inputs, targets = inputs.to(device), targets.to(device)\n                output = model2(inputs)\n                loss = criterion(output, targets)\n                test_loss.append(loss.item())\n        model2.train()\n\n        test_loss = np.mean(test_loss)\n        train_losses[e] = train_loss\n        test_losses[e] = test_loss\n        \n        dt = datetime.now() - t0\n        print(f\"Epoch {e+1}/{epochs} Train loss: {train_loss:.4f} Val loss: {test_loss:.4f} Duration: {dt}\")\n\n    # Memory and timing results\n    avg_inference_time = total_inference_time / inference_count if inference_count > 0 else 0\n    print(f\"\\nTraining Summary:\")\n    print(f\"Average inference time per batch: {avg_inference_time:.4f} seconds\")\n    \n    if torch.cuda.is_available():\n        mem_after = torch.cuda.memory_allocated() / 1024**2\n        mem_used = mem_after - mem_before\n        print(f\"GPU Memory used: {mem_used:.2f} MB\")\n    \n    return train_losses, test_losses","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:36:49.821658Z","iopub.execute_input":"2025-03-27T15:36:49.821971Z","iopub.status.idle":"2025-03-27T15:36:49.831371Z","shell.execute_reply.started":"2025-03-27T15:36:49.821944Z","shell.execute_reply":"2025-03-27T15:36:49.830332Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"train_losses, test_losses= batch_gd_new(model2, criterion, train_loader, validation_loader, 10)\nplot_losses(train_losses, test_losses)","metadata":{"trusted":true},"outputs":[{"name":"stdout","text":"Epoch 1/10 Train loss: 3.1770 Val loss: 1.9959 Duration: 0:01:20.495977\nEpoch 2/10 Train loss: 1.2289 Val loss: 0.7246 Duration: 0:01:01.400397\nEpoch 3/10 Train loss: 0.5353 Val loss: 0.4285 Duration: 0:01:01.529139\nEpoch 4/10 Train loss: 0.3124 Val loss: 0.3449 Duration: 0:01:00.286682\nEpoch 5/10 Train loss: 0.2576 Val loss: 0.2943 Duration: 0:01:00.150772\nEpoch 6/10 Train loss: 0.1985 Val loss: 0.2663 Duration: 0:00:59.996434\nEpoch 7/10 Train loss: 0.1574 Val loss: 0.2392 Duration: 0:00:59.868397\nEpoch 8/10 Train loss: 0.1408 Val loss: 0.2623 Duration: 0:00:59.766058\nEpoch 9/10 Train loss: 0.1204 Val loss: 0.2051 Duration: 0:00:59.978305\nEpoch 10/10 Train loss: 0.0741 Val loss: 0.2375 Duration: 0:00:59.771830\n\nTraining Summary:\nAverage inference time per batch: 0.0117 seconds\nGPU Memory used: 64.31 MB\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"def accuracy(loader):\n    n_correct = 0\n    n_total = 0\n    model2.cuda()\n    for inputs, targets in loader:\n        inputs, targets = inputs.cuda(), targets.cuda()\n        outputs = model2(inputs)\n        #print(outputs)\n        _, predictions = torch.max(outputs, 1)\n        n_correct += (predictions == targets).sum().item()\n        n_total += targets.shape[0]\n\n    acc = n_correct / n_total\n    return acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:47:18.632314Z","iopub.execute_input":"2025-03-27T15:47:18.632629Z","iopub.status.idle":"2025-03-27T15:47:18.636922Z","shell.execute_reply.started":"2025-03-27T15:47:18.632605Z","shell.execute_reply":"2025-03-27T15:47:18.636162Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"train_acc = accuracy(train_loader)\ntest_acc = accuracy(test_loader)\nvalidation_acc = accuracy(validation_loader)\n\nprint(\n    f\"Train Accuracy : {train_acc}\\nTest Accuracy : {test_acc}\\nValidation Accuracy : {validation_acc}\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:47:18.638116Z","iopub.execute_input":"2025-03-27T15:47:18.638417Z","iopub.status.idle":"2025-03-27T15:48:52.155346Z","shell.execute_reply.started":"2025-03-27T15:47:18.638373Z","shell.execute_reply":"2025-03-27T15:48:52.154372Z"}},"outputs":[{"name":"stdout","text":"Train Accuracy : 0.9753943217665615\nTest Accuracy : 0.9250440917107584\nValidation Accuracy : 0.9177693761814745\n","output_type":"stream"}],"execution_count":16}]}