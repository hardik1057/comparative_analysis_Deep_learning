{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4305991,"sourceType":"datasetVersion","datasetId":2536476}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torchvision import datasets, transforms, models  # datsets  , transforms\nfrom torch.utils.data.sampler import SubsetRandomSampler\nimport torch.nn as nn\nfrom datetime import datetime\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport time","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-27T12:15:26.491190Z","iopub.execute_input":"2025-03-27T12:15:26.491492Z","iopub.status.idle":"2025-03-27T12:15:32.437219Z","shell.execute_reply.started":"2025-03-27T12:15:26.491467Z","shell.execute_reply":"2025-03-27T12:15:32.436227Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"transform3 = transforms.Compose([\n    transforms.Resize((299, 299)),  # Resize first (required for some augmentations)\n    transforms.RandomAffine(degrees=10, translate=(0.05, 0.05), shear=5),  # Spatial augmentations\n    transforms.ColorJitter(hue=0.05, saturation=0.05),  # Color augmentations\n    transforms.RandomHorizontalFlip(),  # Flip augmentation\n    transforms.RandomApply([transforms.GaussianBlur(kernel_size=7)], p=0.2),  # Optional blur\n    transforms.ToTensor(),  # Convert to tensor (normalized to [0, 1])\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ResNet stats\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T12:15:32.438457Z","iopub.execute_input":"2025-03-27T12:15:32.438886Z","iopub.status.idle":"2025-03-27T12:15:32.443670Z","shell.execute_reply.started":"2025-03-27T12:15:32.438862Z","shell.execute_reply":"2025-03-27T12:15:32.442762Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"dataset_resnet=datasets.ImageFolder(\"/kaggle/input/mepco-tropic-leaf/MepcoTropicLeaf-V1/Database\", transform=transform3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T12:15:33.815420Z","iopub.execute_input":"2025-03-27T12:15:33.815728Z","iopub.status.idle":"2025-03-27T12:16:08.242461Z","shell.execute_reply.started":"2025-03-27T12:15:33.815702Z","shell.execute_reply":"2025-03-27T12:16:08.241527Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"indices=list(range(len(dataset_resnet))) # dataset ko numerate karne ke liye\nsplit=int(np.floor(0.70*len(dataset_resnet)))\nvalidation=int(np.floor(0.60*split))\n\n# agar tmre pass 100 samples hai\n# toh split=70(ie 70% of the dataset)-> ee use hoga training and validation ke liye; remaining (30)30% used hoga as test set\n# tb validation=42 (ie 60% of the dataset)-> ee use hoga for training ke liye; remaining (28)40% used hoga as Validation set\n# toh phir training= 42%; validation= 28%; test=30%\n\nprint(f\"length of train size : {validation}\")\nprint(f\"length of validation size : {split-validation}\")\nprint(f\"length of test size : {len(dataset_resnet)-split}\")\n\nnp.random.shuffle(indices) # dataset me randomness laane ke liye\n\n# ab actual splitting\ntrain_indices, validation_indices, test_indices = (\n    indices[:validation], # [:5]->0,1,2,3,4\n    indices[validation:split],# [1:3]->1,2\n    indices[split:],# [2:]-> 2,3,4,5,........\n)\n\ntrain_sampler = SubsetRandomSampler(train_indices)\nvalidation_sampler = SubsetRandomSampler(validation_indices)\ntest_sampler = SubsetRandomSampler(test_indices)\n\n# print(list(train_indices))\n# print(list(validation_indices))\n# print(list(test_indices))\n\n# print(list(train_sampler))\n# print(list(validation_sampler))\n# print(list(test_sampler))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T12:16:10.449199Z","iopub.execute_input":"2025-03-27T12:16:10.449570Z","iopub.status.idle":"2025-03-27T12:16:10.456574Z","shell.execute_reply.started":"2025-03-27T12:16:10.449540Z","shell.execute_reply":"2025-03-27T12:16:10.455740Z"}},"outputs":[{"name":"stdout","text":"length of train size : 1585\nlength of validation size : 1058\nlength of test size : 1134\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"targets_size = len(dataset_resnet.class_to_idx)# finding the total unique classes and storing it\nprint(targets_size)\nprint(list(dataset_resnet.class_to_idx.keys()))\nnum_classes_list = list(dataset_resnet.class_to_idx.values())# now numerating them\nprint(num_classes_list)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T12:16:12.652411Z","iopub.execute_input":"2025-03-27T12:16:12.652718Z","iopub.status.idle":"2025-03-27T12:16:12.657639Z","shell.execute_reply.started":"2025-03-27T12:16:12.652696Z","shell.execute_reply":"2025-03-27T12:16:12.656644Z"}},"outputs":[{"name":"stdout","text":"50\n['Asthma Plant.zip', 'Avaram.zip', 'Balloon vine.zip', 'Bellyache bush (Green).zip', 'Benghal dayflower.zip', 'Big Caltrops.zip', 'Black-Honey Shrub.zip', 'Bristly Wild Grape.zip', 'Butterfly Pea.zip', 'Cape Gooseberry.zip', 'Common Wireweed.zip', 'Country Mallow.zip', 'Crown flower.zip', 'Green Chireta.zip', 'Holy Basil.zip', 'Indian CopperLeaf.zip', 'Indian Jujube.zip', 'Indian Sarsaparilla.zip', 'Indian Stinging Nettle.zip', 'Indian Thornapple.zip', 'Indian wormwood.zip', 'Ivy Gourd.zip', 'Kokilaksha.zip', 'Land Caltrops (Bindii).zip', 'Madagascar Periwinkle.zip', 'Madras Pea Pumpkin.zip', 'Malabar Catmint.zip', 'Mexican Mint.zip', 'Mexican Prickly Poppy.zip', 'Mountain Knotgrass.zip', 'Nalta Jute.zip', 'Night blooming Cereus.zip', 'Panicled Foldwing.zip', 'Prickly Chaff Flower.zip', 'Punarnava.zip', 'Purple Fruited Pea Eggplant.zip', 'Purple Tephrosia.zip', 'Rosary Pea.zip', 'Shaggy button weed.zip', 'Small Water Clover.zip', 'Spiderwisp.zip', 'Square Stalked Vine.zip', 'Stinking Passionflower.zip', 'Sweet Basil.zip', 'Sweet flag.zip', 'Tinnevelly Senna.zip', 'Trellis Vine.zip', 'Velvet bean.zip', 'coatbuttons.zip', 'heart-leaved moonseed.zip']\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"model = models.resnet18(pretrained=True)\nmodel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T12:16:14.510702Z","iopub.execute_input":"2025-03-27T12:16:14.510989Z","iopub.status.idle":"2025-03-27T12:16:15.399548Z","shell.execute_reply.started":"2025-03-27T12:16:14.510967Z","shell.execute_reply":"2025-03-27T12:16:15.398636Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 85.5MB/s]\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=1000, bias=True)\n)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"n_features = model.conv1.in_channels #resnet18; number of input features in the first convolutional layer\nn_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T12:16:16.548636Z","iopub.execute_input":"2025-03-27T12:16:16.548922Z","iopub.status.idle":"2025-03-27T12:16:16.553963Z","shell.execute_reply.started":"2025-03-27T12:16:16.548900Z","shell.execute_reply":"2025-03-27T12:16:16.553262Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"3"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device) #checking if GPU is available\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T12:16:19.111889Z","iopub.execute_input":"2025-03-27T12:16:19.112171Z","iopub.status.idle":"2025-03-27T12:16:19.364701Z","shell.execute_reply.started":"2025-03-27T12:16:19.112149Z","shell.execute_reply":"2025-03-27T12:16:19.363951Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=1000, bias=True)\n)"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()  # this include softmax + cross entropy loss\n# calculates the loss during training, which will be later used by backpropagation to imporove the models accuracy\noptimizer = torch.optim.Adam(model.parameters())\n#adam optimiser is used to optimise the models parameters(weights of the model) to minimise the loss and hence increase the accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T12:16:20.913833Z","iopub.execute_input":"2025-03-27T12:16:20.914114Z","iopub.status.idle":"2025-03-27T12:16:20.918943Z","shell.execute_reply.started":"2025-03-27T12:16:20.914092Z","shell.execute_reply":"2025-03-27T12:16:20.918088Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"batch_size = 64\ntrain_loader = torch.utils.data.DataLoader(\n    dataset_resnet, batch_size=batch_size, sampler=train_sampler\n)\ntest_loader = torch.utils.data.DataLoader(\n    dataset_resnet, batch_size=batch_size, sampler=test_sampler\n)\nvalidation_loader = torch.utils.data.DataLoader(\n    dataset_resnet, batch_size=batch_size, sampler=validation_sampler\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T12:16:23.856548Z","iopub.execute_input":"2025-03-27T12:16:23.856882Z","iopub.status.idle":"2025-03-27T12:16:23.861511Z","shell.execute_reply.started":"2025-03-27T12:16:23.856852Z","shell.execute_reply":"2025-03-27T12:16:23.860587Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def batch_gd(model, criterion, train_loader, test_loader, epochs):\n    train_losses = np.zeros(epochs)\n    test_losses = np.zeros(epochs)\n\n    for e in range(epochs):\n\n        t0 = datetime.now()\n        train_loss = []\n        for inputs, targets in train_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            optimizer.zero_grad()\n            output = model(inputs)\n            loss = criterion(output, targets)\n            train_loss.append(loss.item())  # torch to numpy world\n            loss.backward()\n            optimizer.step()\n\n        train_loss = np.mean(train_loss)\n        validation_loss = []\n\n        for inputs, targets in validation_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            output = model(inputs)\n            loss = criterion(output, targets)\n            validation_loss.append(loss.item())  # torch to numpy world\n\n        validation_loss = np.mean(validation_loss)\n        train_losses[e] = train_loss\n        dt = datetime.now() - t0\n        print(f\"Epoch : {e+1}/{epochs} Train_loss:{train_loss:.3f} Duration:{dt}\")\n\n    return train_losses","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_losses_resnet = batch_gd(model, criterion, train_loader, validation_loader, 10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_losses(train_losses, test_losses):\n    plt.figure(figsize=(10, 5))\n    plt.plot(train_losses, label=\"Training Loss\")\n    plt.plot(test_losses, label=\"Validation Loss\")\n    plt.title(\"Training vs Validation Loss\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T12:16:33.192735Z","iopub.execute_input":"2025-03-27T12:16:33.193014Z","iopub.status.idle":"2025-03-27T12:16:33.197674Z","shell.execute_reply.started":"2025-03-27T12:16:33.192993Z","shell.execute_reply":"2025-03-27T12:16:33.196704Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def batch_gd_new(model, criterion, train_loader, test_loader, epochs):\n    train_losses = np.zeros(epochs)\n    test_losses = np.zeros(epochs)\n    \n    # Memory tracking initialization\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        mem_before = torch.cuda.memory_allocated() / 1024**2  # MB\n    \n    # Track average inference time\n    total_inference_time = 0\n    inference_count = 0\n    \n    for e in range(epochs):\n        t0 = datetime.now()\n        train_loss = []\n        \n        # Training phase\n        for inputs, targets in train_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            \n            # Measure inference time\n            start_time = time.time()\n            optimizer.zero_grad()\n            output = model(inputs)\n            inference_time = time.time() - start_time\n            \n            total_inference_time += inference_time\n            inference_count += 1\n            \n            loss = criterion(output, targets)\n            train_loss.append(loss.item())\n            loss.backward()\n            optimizer.step()\n\n        train_loss = np.mean(train_loss)\n        test_loss = []\n        \n        # Validation phase\n        model.eval()\n        with torch.no_grad():\n            for inputs, targets in test_loader:\n                inputs, targets = inputs.to(device), targets.to(device)\n                output = model(inputs)\n                loss = criterion(output, targets)\n                test_loss.append(loss.item())\n        model.train()\n\n        test_loss = np.mean(test_loss)\n        train_losses[e] = train_loss\n        test_losses[e] = test_loss\n        \n        dt = datetime.now() - t0\n        print(f\"Epoch {e+1}/{epochs} Train loss: {train_loss:.4f} Val loss: {test_loss:.4f} Duration: {dt}\")\n\n    # Memory and timing results\n    avg_inference_time = total_inference_time / inference_count if inference_count > 0 else 0\n    print(f\"\\nTraining Summary:\")\n    print(f\"Average inference time per batch: {avg_inference_time:.4f} seconds\")\n    \n    if torch.cuda.is_available():\n        mem_after = torch.cuda.memory_allocated() / 1024**2\n        mem_used = mem_after - mem_before\n        print(f\"GPU Memory used: {mem_used:.2f} MB\")\n    \n    return train_losses, test_losses\n\nplot_losses(train_losses, test_losses)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T12:16:27.094867Z","iopub.execute_input":"2025-03-27T12:16:27.095152Z","iopub.status.idle":"2025-03-27T12:16:27.103613Z","shell.execute_reply.started":"2025-03-27T12:16:27.095129Z","shell.execute_reply":"2025-03-27T12:16:27.102692Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"train_losses_resnet_new = batch_gd_new(model, criterion, train_loader, validation_loader, 10)\nplot_losses(train_losses, test_losses)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T12:16:35.711032Z","iopub.execute_input":"2025-03-27T12:16:35.711342Z","iopub.status.idle":"2025-03-27T12:28:33.465879Z","shell.execute_reply.started":"2025-03-27T12:16:35.711318Z","shell.execute_reply":"2025-03-27T12:28:33.464858Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10 Train loss: 2.7375 Val loss: 5.6522 Duration: 0:01:28.655682\nEpoch 2/10 Train loss: 0.7600 Val loss: 1.1699 Duration: 0:01:10.229502\nEpoch 3/10 Train loss: 0.3501 Val loss: 1.1660 Duration: 0:01:09.551258\nEpoch 4/10 Train loss: 0.2973 Val loss: 0.9291 Duration: 0:01:10.030716\nEpoch 5/10 Train loss: 0.2016 Val loss: 0.3986 Duration: 0:01:09.939124\nEpoch 6/10 Train loss: 0.1528 Val loss: 0.4794 Duration: 0:01:09.930297\nEpoch 7/10 Train loss: 0.1371 Val loss: 0.5685 Duration: 0:01:09.592067\nEpoch 8/10 Train loss: 0.1267 Val loss: 0.6749 Duration: 0:01:09.304474\nEpoch 9/10 Train loss: 0.1231 Val loss: 0.5067 Duration: 0:01:09.995400\nEpoch 10/10 Train loss: 0.0741 Val loss: 0.3826 Duration: 0:01:10.470615\n\nTraining Summary:\nAverage inference time per batch: 0.0081 seconds\nGPU Memory used: 189.02 MB\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-0a55af20b7ec>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_losses_resnet_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_gd_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplot_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'train_losses' is not defined"],"ename":"NameError","evalue":"name 'train_losses' is not defined","output_type":"error"}],"execution_count":13},{"cell_type":"code","source":"plt.plot(train_losses_resnet , label = 'train_loss_vgg')\nplt.xlabel('No of Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()#validation loss ke liye ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def accuracy(loader):\n    n_correct = 0\n    n_total = 0\n    model.cuda()\n    for inputs, targets in loader:\n        inputs, targets = inputs.cuda(), targets.cuda()\n        outputs = model(inputs)\n        #print(outputs)\n        _, predictions = torch.max(outputs, 1)\n        n_correct += (predictions == targets).sum().item()\n        n_total += targets.shape[0]\n\n    acc = n_correct / n_total\n    return acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T12:29:06.462996Z","iopub.execute_input":"2025-03-27T12:29:06.463417Z","iopub.status.idle":"2025-03-27T12:29:06.470024Z","shell.execute_reply.started":"2025-03-27T12:29:06.463375Z","shell.execute_reply":"2025-03-27T12:29:06.468985Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"train_acc = accuracy(train_loader)\ntest_acc = accuracy(test_loader)\nvalidation_acc = accuracy(validation_loader)\n\nprint(\n    f\"Train Accuracy : {train_acc}\\nTest Accuracy : {test_acc}\\nValidation Accuracy : {validation_acc}\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T12:29:06.804637Z","iopub.execute_input":"2025-03-27T12:29:06.804957Z","iopub.status.idle":"2025-03-27T12:30:54.966042Z","shell.execute_reply.started":"2025-03-27T12:29:06.804929Z","shell.execute_reply":"2025-03-27T12:30:54.965095Z"}},"outputs":[{"name":"stdout","text":"Train Accuracy : 0.9735015772870662\nTest Accuracy : 0.9065255731922398\nValidation Accuracy : 0.9092627599243857\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}